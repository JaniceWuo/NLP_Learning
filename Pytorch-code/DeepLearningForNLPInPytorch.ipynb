{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Natural Language Processing with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本教程将向您介绍使用Pytorch进行深度学习编程的关键思想。本教程旨在让您开始编写深度学习代码。\n",
    "请注意，这是关于模型(model)，而不是数据。 对于所有模型，我只创建了一些维度较小的测试示例，以便您可以看到权重在训练时如何变化。 如果您想要尝试一些真实数据，您应该能够从这个笔记本中删除任何模型并在其上使用它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x205d60bc890>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.介绍Torch的张量(tensor)库\n",
    "所有的深度学习都是在张量上计算的。首先，让我们来看看用张量可以做什么。\n",
    "## 创建张量\n",
    "能用torch.Tensor()函数创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: tensor([1, 2, 3])\n",
      "M: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "T: tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "T[0]: tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "#使用给定数据创建torch.Tensor对象。数据是一维的\n",
    "V_data = [1,2,3]\n",
    "V = torch.tensor(V_data)\n",
    "print('V:',V)\n",
    "\n",
    "#创建一个矩阵\n",
    "M_data = [[1,2,3],[4,5,6]]\n",
    "M = torch.tensor(M_data)\n",
    "print('M:',M)\n",
    "\n",
    "#创建一个3维张量，尺寸为2*2*2\n",
    "T_data = [[[1,2], [3,4]],\n",
    "          [[5,6], [7,8]]]\n",
    "T = torch.tensor(T_data)\n",
    "print('T:',T)\n",
    "print('T[0]:',T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002],\n",
      "         [-0.6092, -0.9798, -1.6091, -0.7121,  0.3037],\n",
      "         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284],\n",
      "         [ 0.4676, -0.6970, -1.1608,  0.6995,  0.1991]],\n",
      "\n",
      "        [[ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017],\n",
      "         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177],\n",
      "         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578],\n",
      "         [-0.7735,  0.1991,  0.0457,  0.1530, -0.4757]],\n",
      "\n",
      "        [[-0.1110,  0.2927, -0.1578, -0.0288,  0.4533],\n",
      "         [ 1.1422,  0.2486, -1.7754, -0.0255, -1.0233],\n",
      "         [-0.5962, -1.0055,  0.4285,  1.4761, -1.7869],\n",
      "         [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]])\n"
     ]
    }
   ],
   "source": [
    "#可以用随机的数来创建张量，只需提供维度\n",
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量的一些运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([ 1., 2., 3. ])\n",
    "y = torch.Tensor([ 4., 5., 6. ])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们稍后将使用的一个有用的操作叫连接(concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1153,  0.3170,  0.5629,  0.8662, -0.3528],\n",
      "        [ 0.3482,  1.1371, -0.3339, -1.4724,  0.7296]])\n",
      "tensor([[-0.1312, -0.6368,  1.0429,  0.4903,  1.0318],\n",
      "        [-0.5989,  1.6015, -1.0735, -1.2173,  0.6472],\n",
      "        [-0.0412, -0.1775, -0.5000,  0.8673, -0.2732]])\n",
      "tensor([[-0.1153,  0.3170,  0.5629,  0.8662, -0.3528],\n",
      "        [ 0.3482,  1.1371, -0.3339, -1.4724,  0.7296],\n",
      "        [-0.1312, -0.6368,  1.0429,  0.4903,  1.0318],\n",
      "        [-0.5989,  1.6015, -1.0735, -1.2173,  0.6472],\n",
      "        [-0.0412, -0.1775, -0.5000,  0.8673, -0.2732]])\n",
      "tensor([[-0.4608, -0.0991,  0.4728],\n",
      "        [ 1.0049, -0.2871, -1.1619]])\n",
      "tensor([[ 0.0276,  0.5652, -0.0115,  0.6706, -0.4929],\n",
      "        [ 1.5050, -2.3264,  1.6169, -0.9026,  0.1737]])\n",
      "tensor([[-0.4608, -0.0991,  0.4728,  0.0276,  0.5652, -0.0115,  0.6706, -0.4929],\n",
      "        [ 1.0049, -0.2871, -1.1619,  1.5050, -2.3264,  1.6169, -0.9026,  0.1737]])\n"
     ]
    }
   ],
   "source": [
    "#默认是按照第一轴连接\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 =torch.cat([x_1, y_1])\n",
    "print(x_1)\n",
    "print(y_1)\n",
    "print(z_1)\n",
    "\n",
    "#按照第二轴连接\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(x_2)\n",
    "print(y_2)\n",
    "print(z_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重塑张量\n",
    "使用.view（）方法重塑张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2936, -0.4139, -0.0960, -1.3281],\n",
      "         [ 0.2324,  0.8615,  0.6218, -1.7812],\n",
      "         [-0.9965,  0.8073,  1.1739, -0.9398]],\n",
      "\n",
      "        [[ 0.3861,  1.0473, -0.7327, -0.9168],\n",
      "         [ 0.6867,  0.4209, -1.0214,  0.9886],\n",
      "         [ 0.7806, -2.2049, -1.4975, -0.9023]]])\n",
      "tensor([[ 0.2936, -0.4139, -0.0960, -1.3281,  0.2324,  0.8615,  0.6218, -1.7812,\n",
      "         -0.9965,  0.8073,  1.1739, -0.9398],\n",
      "        [ 0.3861,  1.0473, -0.7327, -0.9168,  0.6867,  0.4209, -1.0214,  0.9886,\n",
      "          0.7806, -2.2049, -1.4975, -0.9023]])\n",
      "tensor([[ 0.2936, -0.4139, -0.0960, -1.3281,  0.2324,  0.8615,  0.6218, -1.7812,\n",
      "         -0.9965,  0.8073,  1.1739, -0.9398],\n",
      "        [ 0.3861,  1.0473, -0.7327, -0.9168,  0.6867,  0.4209, -1.0214,  0.9886,\n",
      "          0.7806, -2.2049, -1.4975, -0.9023]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)   #2*3*4=24\n",
    "print(x)\n",
    "print(x.view(2, 12)) # 2行12列 2*12=24\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.计算图和自动微分\n",
    "计算图的概念对于高效的深度学习编程至关重要，因为它允许您不必自己编写反向传播梯度。\n",
    "计算图只是一个关于如何组合数据以提供输出的规范。 由于图表完全指定了哪些参数涉及哪些操作，因此它包含足够的信息来计算导数。 这可能听起来很模糊，所以让我们看看使用Pytorch的基本类：autograd.Variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([5., 7., 9.])\n",
      "<AddBackward0 object at 0x00000205CED65DD8>\n"
     ]
    }
   ],
   "source": [
    "x = autograd.Variable(torch.tensor([1., 2., 3]), requires_grad=True )  #必须是浮点数\n",
    "print(x.data)\n",
    "y = autograd.Variable(torch.tensor([4., 5., 6]), requires_grad=True )\n",
    "z = x + y\n",
    "print(z.data)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(61.5000, grad_fn=<MeanBackward1>)\n",
      "tensor([12., 15.])\n"
     ]
    }
   ],
   "source": [
    "x = autograd.Variable(torch.tensor([2.,3.]),requires_grad=True)\n",
    "x\n",
    "y = x+2\n",
    "y\n",
    "z = y*y*3\n",
    "z\n",
    "out = z.mean()\n",
    "print(out)\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 用pytorch创建网络组件\n",
    "在我们继续关注NLP之前，让我们做一个注释示例，使用仿射映射和非线性在Pytorch中构建网络。 我们还将看到如何计算损失函数，使用Pytorch内置的负对数似然，并通过反向传播更新参数。\n",
    "让我们编写一个带注释的网络示例，该网络采用稀疏的词袋表示，并在两个标签上输出概率分布：“英语”和“西班牙语”。这个模型只是逻辑回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on': 25, 'la': 4, 'good': 19, 'si': 24, 'sea': 12, 'to': 8, 'Give': 6, 'is': 16, 'get': 20, 'No': 9, 'Yo': 23, 'a': 18, 'una': 13, 'buena': 14, 'que': 11, 'idea': 15, 'comer': 2, 'cafeteria': 5, 'it': 7, 'at': 22, 'lost': 21, 'gusta': 1, 'creo': 10, 'en': 3, 'not': 17, 'me': 0}\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "         (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "         (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "         (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\") ]\n",
    "\n",
    "test_data = [ (\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "              (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module): # inheriting from nn.Module!\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # 确定你理解了为什么输入的维度是vacab_size,输出是num_labels\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0160,  0.0598, -0.1947,  0.1744, -0.0427, -0.1800,  0.1352, -0.0530,\n",
      "          0.1781, -0.1332, -0.1821, -0.1473,  0.0850, -0.0220, -0.1817,  0.0364,\n",
      "         -0.0125,  0.0709, -0.0998, -0.1400,  0.1337, -0.0944,  0.1380,  0.1590,\n",
      "         -0.1372, -0.0904],\n",
      "        [ 0.0236, -0.1849,  0.0602, -0.0817, -0.0870, -0.1516,  0.0917, -0.1250,\n",
      "         -0.1568, -0.0516, -0.1092, -0.1907,  0.0790, -0.0746,  0.1374,  0.1380,\n",
      "         -0.1251, -0.1143, -0.0696,  0.1149, -0.0068, -0.0701,  0.1215,  0.1860,\n",
      "          0.1450, -0.1500]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0407, -0.0376], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 2\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH'), (['Give', 'it', 'to', 'me'], 'ENGLISH'), (['No', 'creo', 'que', 'sea', 'una', 'buena', 'idea'], 'SPANISH'), (['No', 'it', 'is', 'not', 'a', 'good', 'idea', 'to', 'get', 'lost', 'at', 'sea'], 'ENGLISH')]\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([[-0.5896, -0.8086]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_to_ix = { \"SPANISH\": 0, \"ENGLISH\": 1 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来开始训练。为此，我们传递实例以获取日志概率，计算损失函数，计算损失函数的梯度，然后使用渐变步骤更新参数。 损耗函数由Torch在nn包中提供。 nn.NLLLoss()是我们想要的负对数似然丢失。它还定义了torch.optim中的优化函数。在这里，我们将使用SGD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1670, -1.8723]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-2.8061, -0.0623]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# 训练之前在测试集上运行 \n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "# print(next(model.parameters())[:,word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1670, -1.8723]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-2.8061, -0.0623]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # 第一步。\n",
    "        # 因为Pytorch积累梯度值，所以要在每个实例前清空\n",
    "        model.zero_grad()\n",
    "    \n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a Variable\n",
    "        # as an integer.  For example, if the target is SPANISH, then we wrap the integer\n",
    "        # 0.  The loss function then knows that the 0th element of the log probabilities is\n",
    "        # the log probability corresponding to SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "    \n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "    \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
